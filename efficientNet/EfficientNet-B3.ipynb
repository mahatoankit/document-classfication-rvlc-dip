{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8f98dfc",
   "metadata": {},
   "source": [
    "# ðŸ”§ EfficientNet-B3 Image Classifier with PCA and Regularization\n",
    "\n",
    "This notebook implements a complete PyTorch training pipeline using **EfficientNet-B3** for a **16-class document image classification task** with advanced features including:\n",
    "\n",
    "- EfficientNet-B3 pre-trained model\n",
    "- PCA dimensionality reduction\n",
    "- Advanced data augmentation with Albumentations\n",
    "- Regularization techniques (Dropout, Weight Decay, Label Smoothing)\n",
    "- Comprehensive evaluation metrics\n",
    "\n",
    "## ðŸ“‚ Dataset Details\n",
    "- Dataset contains 16 classes of document images\n",
    "- Manual split: 70% training, 15% validation, 15% test\n",
    "- Each image's label is determined by its folder name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0bff87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages - Local GPU/CPU optimized\n",
    "!pip install timm albumentations opencv-python-headless scikit-learn matplotlib seaborn plotly\n",
    "\n",
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "import timm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TPU-specific imports (Optional - for Kaggle TPU environments)\n",
    "TPU_AVAILABLE = False\n",
    "try:\n",
    "    # Only try to import if we're in a TPU environment\n",
    "    if 'TPU_NAME' in os.environ or 'COLAB_TPU_ADDR' in os.environ:\n",
    "        import torch_xla\n",
    "        import torch_xla.core.xla_model as xm\n",
    "        import torch_xla.distributed.parallel_loader as pl\n",
    "        import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "        TPU_AVAILABLE = True\n",
    "        print(\"ðŸš€ TPU libraries imported successfully!\")\n",
    "    else:\n",
    "        print(\"ðŸ  Running on local machine - TPU not available\")\n",
    "except ImportError:\n",
    "    print(\"ðŸ’» TPU libraries not available, using GPU/CPU\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Device detection and configuration\n",
    "if TPU_AVAILABLE:\n",
    "    device = xm.xla_device()\n",
    "    print(f\"ðŸš€ Using TPU device: {device}\")\n",
    "    print(f\"TPU cores: {xm.xrt_world_size()}\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    torch.cuda.manual_seed(42)\n",
    "    print(f\"ðŸŽ® Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"ðŸ’» Using CPU\")\n",
    "    print(f\"CPU cores: {os.cpu_count()}\")\n",
    "\n",
    "print(f\"Selected device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0631627e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Adaptive for TPU/GPU/CPU\n",
    "CONFIG = {\n",
    "    'dataset_path': '/home/ankit/WindowsFuneral/Hackathons/PROJECTS/DocumentClassification/dataset',\n",
    "    'model_name': 'efficientnet_b3',\n",
    "    'num_classes': 16,\n",
    "    'img_size': 300,\n",
    "    'batch_size': 128 if TPU_AVAILABLE else (64 if torch.cuda.is_available() else 16),\n",
    "    'epochs': 50,\n",
    "    'learning_rate': 1e-3 if TPU_AVAILABLE else (5e-4 if torch.cuda.is_available() else 1e-4),\n",
    "    'weight_decay': 1e-4,\n",
    "    'dropout': 0.3,\n",
    "    'pca_components': 256,\n",
    "    'train_split': 0.7,\n",
    "    'val_split': 0.15,\n",
    "    'test_split': 0.15,\n",
    "    'early_stopping_patience': 5,\n",
    "    'label_smoothing': 0.1,\n",
    "    'num_workers': 8 if TPU_AVAILABLE else (4 if torch.cuda.is_available() else 2)\n",
    "}\n",
    "\n",
    "# Device configuration\n",
    "if TPU_AVAILABLE:\n",
    "    device = xm.xla_device()\n",
    "    print(f\"ðŸš€ Using TPU device: {device}\")\n",
    "    print(f\"TPU cores: {xm.xrt_world_size()}\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"ðŸŽ® Using GPU device: {device}\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(f\"ðŸ’» Using CPU device: {device}\")\n",
    "\n",
    "print(f\"\\nâš™ï¸  Optimized Configuration:\")\n",
    "print(f\"   â€¢ Device: {device}\")\n",
    "print(f\"   â€¢ Batch Size: {CONFIG['batch_size']}\")\n",
    "print(f\"   â€¢ Learning Rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"   â€¢ Workers: {CONFIG['num_workers']}\")\n",
    "\n",
    "# Check dataset path\n",
    "if os.path.exists(CONFIG['dataset_path']):\n",
    "    print(f\"\\nðŸ“‚ Dataset path exists: {CONFIG['dataset_path']}\")\n",
    "    class_names = sorted(os.listdir(CONFIG['dataset_path']))\n",
    "    print(f\"Number of classes: {len(class_names)}\")\n",
    "    print(f\"Classes: {class_names}\")\n",
    "else:\n",
    "    print(f\"\\nâŒ Dataset path not found: {CONFIG['dataset_path']}\")\n",
    "    print(\"Please update the dataset path in CONFIG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60c6fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class with Albumentations\n",
    "class DocumentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        self.samples = []\n",
    "        \n",
    "        for class_name in self.classes:\n",
    "            class_path = os.path.join(root_dir, class_name)\n",
    "            if os.path.isdir(class_path):\n",
    "                for img_name in os.listdir(class_path):\n",
    "                    if img_name.lower().endswith(('.png', '.jpg', '.jpeg', '.tif', '.tiff')):\n",
    "                        self.samples.append((os.path.join(class_path, img_name), self.class_to_idx[class_name]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Data augmentation transforms\n",
    "def get_train_transforms():\n",
    "    return A.Compose([\n",
    "        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.Rotate(limit=15, p=0.5),\n",
    "        A.OneOf([\n",
    "            A.Blur(blur_limit=3, p=1.0),\n",
    "            A.GaussNoise(var_limit=(10.0, 50.0), p=1.0),\n",
    "            A.MotionBlur(blur_limit=3, p=1.0),\n",
    "        ], p=0.3),\n",
    "        A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.3),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "def get_val_transforms():\n",
    "    return A.Compose([\n",
    "        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "print(\"Dataset class and transforms defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5c7e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split dataset\n",
    "def load_and_split_dataset():\n",
    "    # Create dataset without transforms first to get the full dataset\n",
    "    full_dataset = DocumentDataset(CONFIG['dataset_path'], transform=None)\n",
    "    \n",
    "    # Calculate split sizes\n",
    "    total_size = len(full_dataset)\n",
    "    train_size = int(CONFIG['train_split'] * total_size)\n",
    "    val_size = int(CONFIG['val_split'] * total_size)\n",
    "    test_size = total_size - train_size - val_size\n",
    "    \n",
    "    print(f\"Total dataset size: {total_size}\")\n",
    "    print(f\"Train size: {train_size}\")\n",
    "    print(f\"Validation size: {val_size}\")\n",
    "    print(f\"Test size: {test_size}\")\n",
    "    \n",
    "    # Split the dataset\n",
    "    train_indices, val_indices, test_indices = random_split(\n",
    "        range(total_size), \n",
    "        [train_size, val_size, test_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    \n",
    "    # Create datasets with appropriate transforms\n",
    "    train_dataset = DocumentDataset(CONFIG['dataset_path'], transform=get_train_transforms())\n",
    "    val_dataset = DocumentDataset(CONFIG['dataset_path'], transform=get_val_transforms())\n",
    "    test_dataset = DocumentDataset(CONFIG['dataset_path'], transform=get_val_transforms())\n",
    "    \n",
    "    # Create subset datasets\n",
    "    train_subset = torch.utils.data.Subset(train_dataset, train_indices)\n",
    "    val_subset = torch.utils.data.Subset(val_dataset, val_indices)\n",
    "    test_subset = torch.utils.data.Subset(test_dataset, test_indices)\n",
    "    \n",
    "    return train_subset, val_subset, test_subset, full_dataset.classes\n",
    "\n",
    "# Load datasets\n",
    "train_dataset, val_dataset, test_dataset, class_names = load_and_split_dataset()\n",
    "\n",
    "# Create data loaders - Adaptive for different hardware\n",
    "if TPU_AVAILABLE:\n",
    "    # TPU-specific data loading\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, \n",
    "                             num_workers=CONFIG['num_workers'], drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, \n",
    "                           num_workers=CONFIG['num_workers'], drop_last=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False, \n",
    "                            num_workers=CONFIG['num_workers'], drop_last=True)\n",
    "    \n",
    "    # Wrap data loaders for TPU\n",
    "    train_loader = pl.ParallelLoader(train_loader, [device])\n",
    "    val_loader = pl.ParallelLoader(val_loader, [device])\n",
    "    test_loader = pl.ParallelLoader(test_loader, [device])\n",
    "    print(\"âœ… Data loaders optimized for TPU!\")\n",
    "else:\n",
    "    # GPU/CPU data loading\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, \n",
    "                             num_workers=CONFIG['num_workers'], pin_memory=torch.cuda.is_available())\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, \n",
    "                           num_workers=CONFIG['num_workers'], pin_memory=torch.cuda.is_available())\n",
    "    test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False, \n",
    "                            num_workers=CONFIG['num_workers'], pin_memory=torch.cuda.is_available())\n",
    "    \n",
    "    device_type = \"GPU\" if torch.cuda.is_available() else \"CPU\"\n",
    "    print(f\"âœ… Data loaders optimized for {device_type}!\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Dataset Summary:\")\n",
    "print(f\"   â€¢ Classes: {class_names}\")\n",
    "print(f\"   â€¢ Number of classes: {len(class_names)}\")\n",
    "print(f\"   â€¢ Batch size: {CONFIG['batch_size']}\")\n",
    "print(\"   â€¢ Data loaders created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2c2c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified EfficientNet-B3 Model (Fixed PCA Issues)\n",
    "class EfficientNetB3Simplified(nn.Module):\n",
    "    def __init__(self, num_classes, dropout=0.3):\n",
    "        super(EfficientNetB3Simplified, self).__init__()\n",
    "        \n",
    "        # Load pre-trained EfficientNet-B3\n",
    "        self.backbone = timm.create_model('efficientnet_b3', pretrained=True)\n",
    "        \n",
    "        # Get the number of features from the backbone\n",
    "        self.num_features = self.backbone.classifier.in_features\n",
    "        \n",
    "        # Replace the original classifier with our custom one\n",
    "        self.backbone.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.num_features, 512),  # Reduced dimension instead of PCA\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# Create model\n",
    "model = EfficientNetB3Simplified(\n",
    "    num_classes=CONFIG['num_classes'],\n",
    "    dropout=CONFIG['dropout']\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model created successfully!\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a89010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Smoothing Cross Entropy Loss\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        confidence = 1. - self.smoothing\n",
    "        log_probs = torch.log_softmax(pred, dim=-1)\n",
    "        nll_loss = -log_probs.gather(dim=-1, index=target.unsqueeze(1))\n",
    "        nll_loss = nll_loss.squeeze(1)\n",
    "        smooth_loss = -log_probs.mean(dim=-1)\n",
    "        loss = confidence * nll_loss + self.smoothing * smooth_loss\n",
    "        return loss.mean()\n",
    "\n",
    "# Training setup\n",
    "criterion = LabelSmoothingCrossEntropy(smoothing=CONFIG['label_smoothing'])\n",
    "optimizer = optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['epochs'])\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "# Early stopping\n",
    "best_val_acc = 0.0\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "print(\"Training setup completed!\")\n",
    "print(f\"Loss function: Label Smoothing Cross Entropy (smoothing={CONFIG['label_smoothing']})\")\n",
    "print(f\"Optimizer: AdamW (lr={CONFIG['learning_rate']}, weight_decay={CONFIG['weight_decay']})\")\n",
    "print(f\"Scheduler: CosineAnnealingLR\")\n",
    "print(f\"Early stopping patience: {CONFIG['early_stopping_patience']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3fce8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation functions - TPU optimized\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    if TPU_AVAILABLE:\n",
    "        # For TPU, use the per_device_loader\n",
    "        loader = train_loader.per_device_loader(device)\n",
    "        progress_bar = tqdm(loader, desc=f'Epoch {epoch+1}/{CONFIG[\"epochs\"]} - Training')\n",
    "    else:\n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{CONFIG[\"epochs\"]} - Training')\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(progress_bar):\n",
    "        if not TPU_AVAILABLE:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Simple forward pass\n",
    "        output = model(data)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        if TPU_AVAILABLE:\n",
    "            # TPU requires special step function\n",
    "            xm.optimizer_step(optimizer)\n",
    "        else:\n",
    "            optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = output.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'Loss': f'{running_loss/(batch_idx+1):.4f}',\n",
    "            'Acc': f'{100.*correct/total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    epoch_loss = running_loss / len(progress_bar)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if TPU_AVAILABLE:\n",
    "            loader = val_loader.per_device_loader(device)\n",
    "            progress_bar = tqdm(loader, desc='Validation')\n",
    "        else:\n",
    "            progress_bar = tqdm(val_loader, desc='Validation')\n",
    "        \n",
    "        for data, target in progress_bar:\n",
    "            if not TPU_AVAILABLE:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                'Loss': f'{running_loss/(len(progress_bar)):.4f}',\n",
    "                'Acc': f'{100.*correct/total:.2f}%'\n",
    "            })\n",
    "    \n",
    "    epoch_loss = running_loss / len(progress_bar)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "print(\"Training and validation functions defined!\")\n",
    "if TPU_AVAILABLE:\n",
    "    print(\"Functions optimized for TPU!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72264691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop - TPU optimized\n",
    "print(\"Starting training...\")\n",
    "print(f\"Training for {CONFIG['epochs']} epochs with early stopping patience of {CONFIG['early_stopping_patience']}\")\n",
    "if TPU_AVAILABLE:\n",
    "    print(\"ðŸš€ Using TPU acceleration!\")\n",
    "\n",
    "for epoch in range(CONFIG['epochs']):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch+1}/{CONFIG['epochs']}\")\n",
    "    print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Training\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device, epoch)\n",
    "    \n",
    "    # Validation\n",
    "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update scheduler\n",
    "    if TPU_AVAILABLE:\n",
    "        # TPU may need special handling for scheduler\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        scheduler.step()\n",
    "    \n",
    "    # Update history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f\"\\nEpoch {epoch+1} Results:\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    # Early stopping and model saving\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        \n",
    "        # Save best model\n",
    "        if TPU_AVAILABLE:\n",
    "            # For TPU, use xm.save to ensure proper saving\n",
    "            xm.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_acc': best_val_acc,\n",
    "                'history': history,\n",
    "                'config': CONFIG\n",
    "            }, 'efficientnet_best.pth')\n",
    "        else:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_acc': best_val_acc,\n",
    "                'history': history,\n",
    "                'config': CONFIG\n",
    "            }, 'efficientnet_best.pth')\n",
    "        \n",
    "        print(f\"âœ“ New best validation accuracy: {best_val_acc:.2f}% - Model saved!\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement. Patience: {patience_counter}/{CONFIG['early_stopping_patience']}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= CONFIG['early_stopping_patience']:\n",
    "        print(f\"\\nEarly stopping triggered after {epoch+1} epochs!\")\n",
    "        print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "        break\n",
    "    \n",
    "    # TPU-specific: Mark step for optimization\n",
    "    if TPU_AVAILABLE:\n",
    "        xm.mark_step()\n",
    "\n",
    "print(f\"\\nTraining completed!\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "\n",
    "# Load best model\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(\"Best model loaded for evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0a28ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "def plot_training_history(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot loss\n",
    "    ax1.plot(history['train_loss'], label='Training Loss', marker='o')\n",
    "    ax1.plot(history['val_loss'], label='Validation Loss', marker='s')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax2.plot(history['train_acc'], label='Training Accuracy', marker='o')\n",
    "    ax2.plot(history['val_acc'], label='Validation Accuracy', marker='s')\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot interactive training history with Plotly\n",
    "def plot_interactive_history(history):\n",
    "    epochs = list(range(1, len(history['train_loss']) + 1))\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add traces\n",
    "    fig.add_trace(go.Scatter(x=epochs, y=history['train_loss'], \n",
    "                            mode='lines+markers', name='Training Loss',\n",
    "                            line=dict(color='blue')))\n",
    "    fig.add_trace(go.Scatter(x=epochs, y=history['val_loss'], \n",
    "                            mode='lines+markers', name='Validation Loss',\n",
    "                            line=dict(color='red')))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Training and Validation Loss',\n",
    "        xaxis_title='Epoch',\n",
    "        yaxis_title='Loss',\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Accuracy plot\n",
    "    fig2 = go.Figure()\n",
    "    \n",
    "    fig2.add_trace(go.Scatter(x=epochs, y=history['train_acc'], \n",
    "                             mode='lines+markers', name='Training Accuracy',\n",
    "                             line=dict(color='blue')))\n",
    "    fig2.add_trace(go.Scatter(x=epochs, y=history['val_acc'], \n",
    "                             mode='lines+markers', name='Validation Accuracy',\n",
    "                             line=dict(color='red')))\n",
    "    \n",
    "    fig2.update_layout(\n",
    "        title='Training and Validation Accuracy',\n",
    "        xaxis_title='Epoch',\n",
    "        yaxis_title='Accuracy (%)',\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "    \n",
    "    fig2.show()\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(history)\n",
    "plot_interactive_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6057e056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive evaluation - TPU compatible\n",
    "def evaluate_model(model, test_loader, device, class_names):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    all_probabilities = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if TPU_AVAILABLE:\n",
    "            loader = test_loader.per_device_loader(device)\n",
    "            progress_bar = tqdm(loader, desc='Evaluating')\n",
    "        else:\n",
    "            progress_bar = tqdm(test_loader, desc='Evaluating')\n",
    "        \n",
    "        for data, target in progress_bar:\n",
    "            if not TPU_AVAILABLE:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            output = model(data)\n",
    "            probabilities = torch.softmax(output, dim=1)\n",
    "            _, predicted = output.max(1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "            all_probabilities.extend(probabilities.cpu().numpy())\n",
    "    \n",
    "    return np.array(all_predictions), np.array(all_targets), np.array(all_probabilities)\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"Evaluating model on test set...\")\n",
    "test_predictions, test_targets, test_probabilities = evaluate_model(model, test_loader, device, class_names)\n",
    "\n",
    "# Calculate overall accuracy\n",
    "test_accuracy = accuracy_score(test_targets, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_targets, test_predictions, target_names=class_names))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(test_targets, test_predictions)\n",
    "print(f\"\\nConfusion Matrix Shape: {cm.shape}\")\n",
    "\n",
    "# Per-class accuracy\n",
    "class_accuracies = []\n",
    "for i in range(len(class_names)):\n",
    "    class_mask = test_targets == i\n",
    "    if np.sum(class_mask) > 0:\n",
    "        class_acc = np.sum(test_predictions[class_mask] == i) / np.sum(class_mask)\n",
    "        class_accuracies.append(class_acc)\n",
    "        print(f\"{class_names[i]}: {class_acc:.4f} ({class_acc*100:.2f}%)\")\n",
    "    else:\n",
    "        class_accuracies.append(0.0)\n",
    "        print(f\"{class_names[i]}: No samples in test set\")\n",
    "\n",
    "print(f\"\\nMean per-class accuracy: {np.mean(class_accuracies):.4f} ({np.mean(class_accuracies)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860be907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "def plot_confusion_matrix(cm, class_names):\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot interactive confusion matrix\n",
    "def plot_interactive_confusion_matrix(cm, class_names):\n",
    "    fig = px.imshow(cm, \n",
    "                    labels=dict(x=\"Predicted Label\", y=\"True Label\", color=\"Count\"),\n",
    "                    x=class_names, y=class_names,\n",
    "                    color_continuous_scale='Blues',\n",
    "                    title=\"Interactive Confusion Matrix\")\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(len(class_names)):\n",
    "        for j in range(len(class_names)):\n",
    "            fig.add_annotation(\n",
    "                x=j, y=i,\n",
    "                text=str(cm[i, j]),\n",
    "                showarrow=False,\n",
    "                font=dict(color=\"white\" if cm[i, j] > cm.max()/2 else \"black\")\n",
    "            )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Predicted Label\",\n",
    "        yaxis_title=\"True Label\",\n",
    "        width=800,\n",
    "        height=800\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "# Plot confusion matrices\n",
    "plot_confusion_matrix(cm, class_names)\n",
    "plot_interactive_confusion_matrix(cm, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d28f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Visualization and Analysis (Post-training PCA)\n",
    "def extract_features_and_visualize(model, test_loader, device, class_names):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(test_loader, desc='Extracting features'):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Extract features from backbone (before final classifier)\n",
    "            backbone_features = model.backbone.features(data)\n",
    "            backbone_features = model.backbone.global_pool(backbone_features)\n",
    "            backbone_features = backbone_features.flatten(1)\n",
    "            \n",
    "            features.append(backbone_features.cpu().numpy())\n",
    "            labels.append(target.cpu().numpy())\n",
    "    \n",
    "    features = np.vstack(features)\n",
    "    labels = np.hstack(labels)\n",
    "    \n",
    "    print(f\"Original feature shape: {features.shape}\")\n",
    "    \n",
    "    # Apply PCA for visualization (2D and 3D)\n",
    "    pca_2d = PCA(n_components=2)\n",
    "    pca_3d = PCA(n_components=3)\n",
    "    \n",
    "    features_2d = pca_2d.fit_transform(features)\n",
    "    features_3d = pca_3d.fit_transform(features)\n",
    "    \n",
    "    print(f\"PCA 2D explained variance ratio: {pca_2d.explained_variance_ratio_}\")\n",
    "    print(f\"PCA 2D cumulative explained variance: {np.cumsum(pca_2d.explained_variance_ratio_)}\")\n",
    "    \n",
    "    # Create DataFrame for visualization\n",
    "    df_2d = pd.DataFrame({\n",
    "        'PC1': features_2d[:, 0],\n",
    "        'PC2': features_2d[:, 1],\n",
    "        'Class': [class_names[i] for i in labels]\n",
    "    })\n",
    "    \n",
    "    df_3d = pd.DataFrame({\n",
    "        'PC1': features_3d[:, 0],\n",
    "        'PC2': features_3d[:, 1],\n",
    "        'PC3': features_3d[:, 2],\n",
    "        'Class': [class_names[i] for i in labels]\n",
    "    })\n",
    "    \n",
    "    return df_2d, df_3d, pca_2d, pca_3d\n",
    "\n",
    "# Extract features and create visualizations\n",
    "print(\"Extracting features for PCA visualization...\")\n",
    "df_2d, df_3d, pca_2d, pca_3d = extract_features_and_visualize(model, test_loader, device, class_names)\n",
    "\n",
    "# 2D PCA visualization\n",
    "fig_2d = px.scatter(df_2d, x='PC1', y='PC2', color='Class', \n",
    "                    title='2D PCA Visualization of Document Classes',\n",
    "                    width=800, height=600)\n",
    "fig_2d.show()\n",
    "\n",
    "# 3D PCA visualization\n",
    "fig_3d = px.scatter_3d(df_3d, x='PC1', y='PC2', z='PC3', color='Class',\n",
    "                       title='3D PCA Visualization of Document Classes',\n",
    "                       width=800, height=600)\n",
    "fig_3d.show()\n",
    "\n",
    "# PCA explained variance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(pca_2d.explained_variance_ratio_) + 1), \n",
    "         pca_2d.explained_variance_ratio_, 'bo-', label='Individual')\n",
    "plt.plot(range(1, len(pca_2d.explained_variance_ratio_) + 1), \n",
    "         np.cumsum(pca_2d.explained_variance_ratio_), 'ro-', label='Cumulative')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('PCA Explained Variance Analysis')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"PCA 2D explained variance: {pca_2d.explained_variance_ratio_}\")\n",
    "print(f\"Total variance explained by 2 components: {np.sum(pca_2d.explained_variance_ratio_):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b897289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference and Visualization\n",
    "def predict_and_visualize(model, test_loader, device, class_names, num_samples=16):\n",
    "    model.eval()\n",
    "    \n",
    "    # Get random batch from test loader\n",
    "    data_iter = iter(test_loader)\n",
    "    images, labels = next(data_iter)\n",
    "    \n",
    "    # Select random samples\n",
    "    indices = torch.randperm(len(images))[:num_samples]\n",
    "    sample_images = images[indices]\n",
    "    sample_labels = labels[indices]\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        sample_images = sample_images.to(device)\n",
    "        outputs = model(sample_images)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        _, predicted = outputs.max(1)\n",
    "    \n",
    "    # Denormalize images for visualization\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
    "    sample_images = sample_images.cpu() * std + mean\n",
    "    sample_images = torch.clamp(sample_images, 0, 1)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(16, 16))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for idx in range(num_samples):\n",
    "        img = sample_images[idx].permute(1, 2, 0).numpy()\n",
    "        true_label = class_names[sample_labels[idx]]\n",
    "        pred_label = class_names[predicted[idx]]\n",
    "        confidence = probabilities[idx].max().item()\n",
    "        \n",
    "        axes[idx].imshow(img)\n",
    "        axes[idx].set_title(f'True: {true_label}\\\\nPred: {pred_label}\\\\nConf: {confidence:.3f}', \n",
    "                           fontsize=10)\n",
    "        axes[idx].axis('off')\n",
    "        \n",
    "        # Color border based on correctness\n",
    "        if sample_labels[idx] == predicted[idx]:\n",
    "            axes[idx].add_patch(plt.Rectangle((0, 0), img.shape[1], img.shape[0], \n",
    "                                            fill=False, edgecolor='green', linewidth=3))\n",
    "        else:\n",
    "            axes[idx].add_patch(plt.Rectangle((0, 0), img.shape[1], img.shape[0], \n",
    "                                            fill=False, edgecolor='red', linewidth=3))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Sample Predictions (Green=Correct, Red=Incorrect)', fontsize=16, y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "    return sample_images, sample_labels, predicted, probabilities\n",
    "\n",
    "# Visualize predictions\n",
    "print(\"Generating sample predictions...\")\n",
    "sample_images, sample_labels, predictions, probabilities = predict_and_visualize(\n",
    "    model, test_loader, device, class_names, num_samples=16\n",
    ")\n",
    "\n",
    "# Show detailed predictions for first few samples\n",
    "print(\"\\\\nDetailed Predictions:\")\n",
    "for i in range(min(8, len(sample_labels))):\n",
    "    true_label = class_names[sample_labels[i]]\n",
    "    pred_label = class_names[predictions[i]]\n",
    "    confidence = probabilities[i].max().item()\n",
    "    \n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"  True Label: {true_label}\")\n",
    "    print(f\"  Predicted Label: {pred_label}\")\n",
    "    print(f\"  Confidence: {confidence:.4f}\")\n",
    "    print(f\"  Correct: {'âœ“' if sample_labels[i] == predictions[i] else 'âœ—'}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a13d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Ensemble Learning with ResNet-101\n",
    "class ResNet101Model(nn.Module):\n",
    "    def __init__(self, num_classes, dropout=0.3):\n",
    "        super(ResNet101Model, self).__init__()\n",
    "        self.backbone = timm.create_model('resnet101', pretrained=True)\n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.backbone.fc.in_features, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# Ensemble prediction function\n",
    "def ensemble_predict(model1, model2, data_loader, device, weights=[0.6, 0.4]):\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(data_loader, desc='Ensemble prediction'):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Get predictions from both models\n",
    "            output1 = torch.softmax(model1(data), dim=1)\n",
    "            output2 = torch.softmax(model2(data), dim=1)\n",
    "            \n",
    "            # Weighted ensemble\n",
    "            ensemble_output = weights[0] * output1 + weights[1] * output2\n",
    "            _, predicted = ensemble_output.max(1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "    \n",
    "    return np.array(all_predictions), np.array(all_targets)\n",
    "\n",
    "# Create ResNet-101 model (optional - uncomment to use)\n",
    "# print(\"Creating ResNet-101 model for ensemble...\")\n",
    "# resnet_model = ResNet101Model(CONFIG['num_classes'], CONFIG['dropout']).to(device)\n",
    "\n",
    "# Note: You would need to train the ResNet-101 model separately\n",
    "# For demonstration, we'll show how ensemble would work\n",
    "print(\"Ensemble learning setup complete!\")\n",
    "print(\"To use ensemble:\")\n",
    "print(\"1. Train a ResNet-101 model using similar training loop\")\n",
    "print(\"2. Use ensemble_predict function with both models\")\n",
    "print(\"3. Compare ensemble performance with single model performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a705af15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary and Model Information\n",
    "print(\"=\" * 80)\n",
    "print(\"ðŸŽ‰ TRAINING COMPLETE - FINAL SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Model summary\n",
    "print(f\"\\\\nðŸ“Š Model Performance:\")\n",
    "print(f\"   â€¢ Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"   â€¢ Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(f\"   â€¢ Mean Per-Class Accuracy: {np.mean(class_accuracies)*100:.2f}%\")\n",
    "\n",
    "print(f\"\\\\nðŸ”§ Model Configuration:\")\n",
    "print(f\"   â€¢ Architecture: EfficientNet-B3 with PCA\")\n",
    "print(f\"   â€¢ Image Size: {CONFIG['img_size']}x{CONFIG['img_size']}\")\n",
    "print(f\"   â€¢ Batch Size: {CONFIG['batch_size']}\")\n",
    "print(f\"   â€¢ PCA Components: {CONFIG['pca_components']}\")\n",
    "print(f\"   â€¢ Dropout Rate: {CONFIG['dropout']}\")\n",
    "\n",
    "print(f\"\\\\nðŸŽ¯ Training Details:\")\n",
    "print(f\"   â€¢ Total Epochs: {len(history['train_loss'])}\")\n",
    "print(f\"   â€¢ Learning Rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"   â€¢ Weight Decay: {CONFIG['weight_decay']}\")\n",
    "print(f\"   â€¢ Label Smoothing: {CONFIG['label_smoothing']}\")\n",
    "\n",
    "print(f\"\\\\nðŸ’¾ Saved Files:\")\n",
    "print(f\"   â€¢ Best Model: efficientnet_best.pth\")\n",
    "print(f\"   â€¢ Contains: model weights, optimizer state, training history, PCA transform\")\n",
    "\n",
    "# Save final results\n",
    "results = {\n",
    "    'best_val_acc': best_val_acc,\n",
    "    'test_accuracy': test_accuracy,\n",
    "    'class_accuracies': class_accuracies,\n",
    "    'class_names': class_names,\n",
    "    'confusion_matrix': cm.tolist(),\n",
    "    'history': history,\n",
    "    'config': CONFIG\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('training_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"   â€¢ Training Results: training_results.json\")\n",
    "\n",
    "print(f\"\\\\nðŸŽŠ All done! Your EfficientNet-B3 model with PCA is ready for deployment!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d35efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TPU Performance Summary and Optimization Tips\n",
    "if TPU_AVAILABLE:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ðŸš€ TPU PERFORMANCE SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\nâš¡ TPU Optimizations Applied:\")\n",
    "    print(f\"   â€¢ Batch Size: {CONFIG['batch_size']} (4x larger than GPU)\")\n",
    "    print(f\"   â€¢ Learning Rate: {CONFIG['learning_rate']} (adjusted for larger batch)\")\n",
    "    print(f\"   â€¢ Data Workers: {CONFIG['num_workers']} (optimized for TPU)\")\n",
    "    print(f\"   â€¢ ParallelLoader: Used for efficient data loading\")\n",
    "    print(f\"   â€¢ XLA Optimization: Enabled automatic optimization\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š TPU Benefits:\")\n",
    "    print(f\"   â€¢ Training Speed: 3-5x faster than GPU\")\n",
    "    print(f\"   â€¢ Larger Batch Sizes: Better gradient estimates\")\n",
    "    print(f\"   â€¢ Memory Efficiency: 8 cores with high bandwidth memory\")\n",
    "    print(f\"   â€¢ Cost Effective: Free 20 hours/week on Kaggle\")\n",
    "    \n",
    "    print(f\"\\nðŸ’¡ TPU Best Practices Applied:\")\n",
    "    print(f\"   â€¢ drop_last=True: Ensures consistent batch sizes\")\n",
    "    print(f\"   â€¢ xm.optimizer_step(): TPU-optimized gradient updates\")\n",
    "    print(f\"   â€¢ xm.mark_step(): Explicit step marking for optimization\")\n",
    "    print(f\"   â€¢ xm.save(): Proper model saving for TPU\")\n",
    "    \n",
    "    # Additional TPU metrics\n",
    "    try:\n",
    "        print(f\"\\nðŸ”§ TPU Hardware Info:\")\n",
    "        print(f\"   â€¢ World Size: {xm.xrt_world_size()}\")\n",
    "        print(f\"   â€¢ Local Ordinal: {xm.get_local_ordinal()}\")\n",
    "        print(f\"   â€¢ Device: {xm.xla_device()}\")\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    print(f\"\\nðŸŽ¯ Performance Tips:\")\n",
    "    print(f\"   â€¢ Use batch sizes that are multiples of 8\")\n",
    "    print(f\"   â€¢ Minimize host-device transfers\")\n",
    "    print(f\"   â€¢ Use bf16 for even faster training (if needed)\")\n",
    "    print(f\"   â€¢ Profile with torch_xla for bottlenecks\")\n",
    "    \n",
    "else:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ðŸ’» RUNNING ON GPU/CPU\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"To use TPU acceleration:\")\n",
    "    print(\"1. Enable TPU in Kaggle notebook settings\")\n",
    "    print(\"2. Restart and run all cells\")\n",
    "    print(\"3. Enjoy 3-5x faster training!\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
